# A. 연쇄 법칙과 역전파: 딥러닝은 어떻게 '책임'을 전가하여 학습하는가?

"다변수 연쇄 법칙은 역전파(Backpropagation)의 수학적 엔진이다."


## 1. 문제 상황: 거대한 '함수 공장'과 원인 불명의 불량품

딥러닝 신경망은 수많은 함수가 중첩된 '거대한 함수 공장'과 같습니다.

* **공정(Layer)**: 1층, 2층, ..., N층의 공정이 순서대로 있습니다.
* **작업자(Weight)**: 각 공정에는 수많은 작업자(가중치, $w$)들이 있습니다.
* **중간 부품(Activation)**: 1층 공정에서 나온 중간 부품($a_1$)은 2층으로, 2층에서 나온 중간 부품($a_2$)은 3층으로 전달됩니다.
* **최종 제품(Output)**: 마지막 공정에서 최종 제품($\hat{y}$)이 나옵니다.
* **고객 불만(Loss)**: 그런데 이 최종 제품이 고객이 원하던 것($y$)과 달라 불만이 접수되었습니다. 이 불만 지수가 바로 **손실(Loss, $L$)** 입니다.

**공장장의 목표**: 이 불량품(Loss)의 원인을 찾아내야 합니다. 수많은 공정의 수많은 작업자들 중, **"대체 누가, 얼마나 잘못했는가?"** 를 알아내야 그 작업자들을 교육(가중치 수정)시켜 문제를 해결할 수 있습니다.

즉, 우리는 공장 깊숙이 숨어있는 어떤 작업자($w_k$)가 최종 불만 지수($L$)에 얼마나 영향을 미쳤는지, 그 '책임의 정도'인 **$\frac{\partial L}{\partial w_k}$** 를 계산해야 합니다.

하지만 $w_k$는 최종 제품이 나오기 한참 전의 중간 공정에 있습니다. 여기서 바로 연쇄 법칙이 등장합니다.


## 2. 연쇄 법칙: '책임'을 연쇄적으로 묻는 수학적 방법

연쇄 법칙의 본질은 **"A가 B에 미치는 영향은, (A가 C에 미치는 영향) × (C가 B에 미치는 영향)으로 계산할 수 있다"** 는 것입니다.

가장 간단한 1차원 연쇄 법칙은 다음과 같습니다.
> 만약 $y = f(u)$ 이고 $u = g(x)$ 이면, $x$가 변할 때 $y$가 얼마나 변하는지($\frac{dy}{dx}$)는 아래와 같이 계산할 수 있다.
> $$ \frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx} $$

이것을 우리 '함수 공장'에 적용하는 것이 바로 **다변수 연쇄 법칙**이고, 이 법칙을 따라 책임을 묻는 과정이 **역전파(Backpropagation)** 입니다.


## 3. 역전파: 연쇄 법칙을 이용한 '책임 전가' 프로세스

공장장은 불량의 원인을 찾기 위해 생산 라인의 맨 처음부터 조사하지 않습니다. 가장 효율적인 방법은 **맨 끝, 즉 고객 불만이 접수된 곳에서부터 거꾸로(Back)** 조사하는 것입니다.

#### 1단계: 최종 공정(출력층)의 책임을 계산한다.

* 먼저, 최종 제품을 만든 마지막 공정($L$층)의 작업자들($w_L$)이 최종 불만 지수($L$)에 얼마나 영향을 미쳤는지($\frac{\partial L}{\partial w_L}$)를 계산합니다. 이것은 바로 붙어있기 때문에 계산이 비교적 쉽습니다.
* 또한, 마지막 공정에 전달된 **'바로 전 단계의 부품'($a_{L-1}$)** 이 최종 불만 지수에 얼마나 영향을 미쳤는지도 계산합니다. 이것이 바로 뒤 공정으로 책임을 넘길 **'오차 신호(error signal)'**, 즉 $\frac{\partial L}{\partial a_{L-1}}$ 입니다.

#### 2단계: 연쇄 법칙으로 '오차 신호'를 뒤로 전달한다.

* 이제 $L-1$층 공정으로 이동합니다. 공장장은 방금 계산한 '오차 신호'($\frac{\partial L}{\partial a_{L-1}}$)를 들고 가서 말합니다.
    > "이봐, $L$층에서 불량이 났는데, 너희가 넘겨준 부품($a_{L-1}$)이 원인의 일부야! 그 책임의 정도가 바로 $\frac{\partial L}{\partial a_{L-1}}$ 만큼이야."
* 이것이 바로 **오차를 뒤로 전파(Back-propagate)** 하는 과정입니다.

#### 3단계: 현재 공정의 책임을 계산하고, 다시 뒤로 전달한다.

* 이제 $L-1$층은 자신들에게 전가된 책임($\frac{\partial L}{\partial a_{L-1}}$)을 기반으로, 자신들의 작업자($w_{L-1}$)들이 얼마나 잘못했는지 계산합니다. 연쇄 법칙에 따라 다음과 같이 계산됩니다.
    > $$ \frac{\partial L}{\partial w_{L-1}} = \frac{\partial L}{\partial a_{L-1}} \cdot \frac{\partial a_{L-1}}{\partial w_{L-1}} $$
    > `(최종 오차에 대한 나의 책임)` = `(뒷 공정에서 나에게 넘긴 책임)` × `(내부적으로 내가 잘못한 정도)`
* 그리고 동시에, 이들은 자신들의 잘못 중 일부는 바로 앞 공정($L-2$층)에서 넘어온 부품($a_{L-2}$) 탓이라며, 또다시 연쇄 법칙을 이용해 오차 신호 $\frac{\partial L}{\partial a_{L-2}}$를 계산하여 뒤로 넘깁니다.

#### 4단계: 1층에 도달할 때까지 반복한다.

이 '오차 신호 전달 → 현재 층 책임 계산' 과정을 입력층에 도달할 때까지 계속 반복합니다. 이 모든 과정이 끝나면, 우리는 공장 내 **모든 작업자($w$)** 각각이 최종 불량품($L$)에 대해 얼마나 책임이 있는지, 즉 모든 $\frac{\partial L}{\partial w}$ 값을 알게 됩니다. 이것이 바로 우리가 원했던 **그래디언트(Gradient)** 입니다.


## 4. 결론: 왜 연쇄 법칙이 중요한가?

* 딥러닝 신경망은 **매우 깊게 중첩된 합성 함수**이기 때문에, 손실 함수 $L$에 대한 각 가중치 $w$의 영향을 한 번에 계산하기가 거의 불가능합니다.
* **연쇄 법칙**은 이 복잡한 함수의 미분을 **효율적으로 계산할 수 있는 유일한 수학적 도구**입니다.
* 특히, 역전파는 연쇄 법칙을 이용하여 **'뒷 공정에서 계산된 미분값(오차 신호)'을 재활용하여 앞 공정의 미분값을 계산**하는 재귀적이고 매우 영리한 방식을 사용합니다. 이 덕분에 계산량이 폭발적으로 증가하는 것을 막을 수 있습니다.

결론적으로, **연쇄 법칙이 없으면 역전파 알고리즘은 존재할 수 없으며, 딥러닝 모델을 효율적으로 학습시키는 것이 불가능**합니다. 연쇄 법칙은 딥러닝이라는 거대한 공장을 움직이는 보이지 않는 핵심 '엔진'인 셈입니다.



# B. neural network에서 backpropagation이 어떻게 일어나는지 보자

딥러닝의 학습, 즉 역전파 과정은 '결과(오차)에 대한 원인(각 가중치)의 기여도를 찾아나가는 여정'입니다. 이 여정의 유일한 나침반이 바로 **연쇄 법칙(Chain Rule)** 입니다.

## 1. 우리가 사용할 '미니 신경망' 모델

계산을 위해 세상에서 가장 간단한 신경망 중 하나를 가정하겠습니다.

* **구조**: 입력 노드 1개 → 은닉층 노드 2개 → 출력 노드 1개
    * `(Input: x)` → `[Hidden: h1, h2]` → `(Output: ŷ)`
* **활성화 함수**: **시그모이드(Sigmoid)** 함수를 사용합니다.
    * $\sigma(z) = \frac{1}{1+e^{-z}}$
    * 시그모이드의 도함수는 $\sigma'(z) = \sigma(z)(1-\sigma(z))$ 로, 출력을 이용해 쉽게 계산할 수 있습니다.
* **손실 함수**: **평균 제곱 오차(Mean Squared Error, MSE)** 를 사용합니다. 계산 편의를 위해 $\frac{1}{2}$을 곱해줍니다.
    * $L = \frac{1}{2}(y - \hat{y})^2$
* **초기값 설정**:
    * **입력값**: $x = 2$
    * **정답값**: $y = 1$
    * **가중치(W)와 편향(b)**: 모두 임의의 값으로 시작합니다.
        * $w_1=0.5, w_2=0.6, w_3=0.8, w_4=0.9$
        * $b_1=0.1, b_2=0.2, b_3=0.3$
    * **학습률(Learning Rate, $\eta$)**: $\eta = 0.5$


## 2. 순전파 (Forward Propagation): 예측값 계산하기

입력값 $x=2$가 신경망을 통과하여 최종 예측값 $\hat{y}$이 되는 과정을 계산합니다.

### 2-1. 은닉층 계산

* **h1 노드**:
    * 입력: $z_{h1} = w_1 \cdot x + b_1 = (0.5 \times 2) + 0.1 = 1.1$
    * 출력: $a_{h1} = \sigma(z_{h1}) = \sigma(1.1) \approx 0.750$
* **h2 노드**:
    * 입력: $z_{h2} = w_2 \cdot x + b_2 = (0.6 \times 2) + 0.2 = 1.4$
    * 출력: $a_{h2} = \sigma(z_{h2}) = \sigma(1.4) \approx 0.802$

### 2-2. 출력층 계산

* **출력 노드**:
    * 입력: $z_{out} = w_3 \cdot a_{h1} + w_4 \cdot a_{h2} + b_3 = (0.8 \times 0.750) + (0.9 \times 0.802) + 0.3 = 0.6 + 0.722 + 0.3 = 1.622$
    * 최종 예측값: $\hat{y} = \sigma(z_{out}) = \sigma(1.622) \approx 0.835$

### 2-3. 오차(Loss) 계산

* 최종 예측값($\hat{y}=0.835$)과 정답($y=1$)의 오차를 계산합니다.
* $L = \frac{1}{2}(y - \hat{y})^2 = \frac{1}{2}(1 - 0.835)^2 = \frac{1}{2}(0.165)^2 \approx 0.0136$

> **현재 우리 모델의 오차는 약 0.0136 입니다. 이제 이 오차의 원인을 거꾸로 추적해 각 가중치를 수정합니다.**


## 3. 역전파 (Backpropagation): '책임'을 계산하고 가중치 수정하기

오차 $L$에 대해 각 가중치가 얼마나 영향을 미쳤는지($\frac{\partial L}{\partial w}$)를 연쇄 법칙을 이용해 계산합니다. **가장 마지막 가중치인 $w_3$부터 시작하겠습니다.**

### 3-1. 출력층 가중치($w_3$)의 '책임'($\frac{\partial L}{\partial w_3}$) 계산

$w_3$가 변하면 $z_{out}$이 변하고, $z_{out}$이 변하면 $\hat{y}$이 변하고, $\hat{y}$이 변하면 최종 오차 $L$이 변합니다. 이 관계를 연쇄 법칙으로 표현하면 다음과 같습니다.

$$
\frac{\partial L}{\partial w_3} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_{out}} \cdot \frac{\partial z_{out}}{\partial w_3}
$$

- $\frac{\partial L}{\partial w_3}$
    - 가중치 w3는 최종 오차에 얼마나 책임이 있냐?
    - 근데 w3은 최종 오차(L)에 직접 영향을 주는 게 아니라, zout을 바꾸고, 그게 y^을 바꾸고, 그게 또 L을 바꾸는 식으로 한 다리 건너 아는 사이잖음? 그래서 책임을 연쇄적으로 묻는 거임. 이게 연쇄 법칙(Chain Rule)임.

$$\frac{\text{최종 오차}}{\text{w 
3
​
 }} = \frac{\text{최종 오차}}{\text{예측값}} \times \frac{\text{예측값}}{\text{출력층 입력}} \times \frac{\text{출력층 입력}}{\text{w 
3
​
 }}$$

---
이제 각 조각을 계산해 봅시다.

1.  **$\frac{\partial L}{\partial \hat{y}}$ (오차/예측값)**: 
    - 그래서 최종적으로 얼마나 오차가 생겼는가?
    - $L = \frac{1}{2}(y - \hat{y})^2$를 $\hat{y}$로 미분하면 $-(y - \hat{y})$가 됩니다.
    * 값: $-(1 - 0.835) = -0.165$

2.  **$\frac{\partial \hat{y}}{\partial z_{out}}$ (예측값/입력)**: 
    - 예측값 y^은 얼마나 민감하게 반응했는가?
    - 근데 예측값(y^)은 z_out 이란 놈을 시그모이드 함수에 넣어서 나온 거잖음? 이 시그모이드란 놈이 받은 값을 얼마나 예민하게 결과로 뱉었냐 이거임. 시그모이드 그래프 중간 부분은 경사가 가팔라서 존나 예민하고, 양쪽 끝으로 가면 경사가 0에 가까워서 존나 둔감함.
    - $\hat{y} = \sigma(z_{out})$를 $z_{out}$로 미분하면 $\sigma'(z_{out}) = \hat{y}(1-\hat{y})$ 입니다.
    * 값: $0.835 \times (1 - 0.835) \approx 0.137$
    - 지금 값은 어중간한 데 있으니 0.137만큼의 민감도를 가짐. 즉, 이만큼 책임을 증폭시키거나 감소시킬 수 있다는 소리.

3.  **$\frac{\partial z_{out}}{\partial w_3}$ (입력/가중치)**: 
    - 그래서 w3가 loss에 기여한건 얼만큼?
    - $z_{out} = w_3 a_{h1} + w_4 a_{h2} + b_3$를 $w_3$로 미분하면 $a_{h1}$만 남습니다.
    * 값: $a_{h1} = 0.750$

이제 이 세 조각을 모두 곱합니다.

> $\frac{\partial L}{\partial w_3} = (-0.165) \times (0.137) \times (0.750) \approx -0.017$

이 값 **-0.017**이 바로 최종 오차에 대한 $w_3$의 책임량, 즉 그래디언트입니다.

책임이 마이너스니까 w3값을 살짝 올려주면 최종 오차가 줄어들 거라는 소리임. 그래서 경사 하강법에서 w = w - (학습률 * 그래디언트) 이렇게 빼주는 거. 

### 3-2. '오차 신호'를 은닉층으로 전달하기

이제 한 단계 더 뒤로 가서 $w_1$의 책임을 계산해야 합니다. 그러려면 먼저, **최종 오차 $L$이 은닉층의 출력값 $a_{h1}$에 대해 얼마나 민감한지($\frac{\partial L}{\partial a_{h1}}$)** 알아야 합니다. 이 값이 바로 뒤로 전달되는 '오차 신호'입니다.

$$
\frac{\partial L}{\partial a_{h1}} = \underbrace{\frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z_{out}}}_{\frac{\partial L}{\partial z_{out}}} \cdot \frac{\partial z_{out}}{\partial a_{h1}}
$$

1.  **$\frac{\partial L}{\partial z_{out}}$ (오차/출력층 입력)**: 이 값은 위에서 계산한 1, 2번 조각의 곱입니다.
    * 값: $(-0.165) \times (0.137) \approx -0.0226$
2.  **$\frac{\partial z_{out}}{\partial a_{h1}}$ (출력층 입력/은닉층 출력)**: $z_{out} = w_3 a_{h1} + ...$ 를 $a_{h1}$로 미분하면 $w_3$만 남습니다.
    * 값: $w_3 = 0.8$

둘을 곱하면 $a_{h1}$에게 전달될 오차 신호가 나옵니다.

> $\frac{\partial L}{\partial a_{h1}} = (-0.0226) \times (0.8) \approx -0.0181$

### 3-3. 은닉층 가중치($w_1$)의 '책임'($\frac{\partial L}{\partial w_1}$) 계산

이제 $w_1$의 책임을 계산할 모든 준비가 끝났습니다. 연쇄 법칙을 다시 적용합니다.

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial a_{h1}} \cdot \frac{\partial a_{h1}}{\partial z_{h1}} \cdot \frac{\partial z_{h1}}{\partial w_1}
$$

1.  **$\frac{\partial L}{\partial a_{h1}}$**: 방금 전 단계에서 뒤로부터 전달받은 '오차 신호'입니다.
    * 값: $-0.0181$
2.  **$\frac{\partial a_{h1}}{\partial z_{h1}}$**: $a_{h1} = \sigma(z_{h1})$를 미분하면 $a_{h1}(1-a_{h1})$ 입니다.
    * 값: $0.750 \times (1 - 0.750) = 0.1875$
3.  **$\frac{\partial z_{h1}}{\partial w_1}$**: $z_{h1} = w_1 x + b_1$를 $w_1$로 미분하면 $x$만 남습니다.
    * 값: $x = 2$

이제 이 세 조각을 모두 곱합니다.

> $\frac{\partial L}{\partial w_1} = (-0.0181) \times (0.1875) \times (2) \approx -0.0068$

이 값 **-0.0068**이 최종 오차에 대한 $w_1$의 책임량입니다.


## 4. 가중치 업데이트: 드디어 '학습'

이제 계산된 책임의 정도(그래디언트)를 이용해 가중치를 수정합니다. (학습률 $\eta=0.5$)

* **$w_3$ 업데이트**:
    * $w_3^{new} = w_3 - \eta \cdot \frac{\partial L}{\partial w_3} = 0.8 - (0.5 \times -0.017) = 0.8 + 0.0085 = 0.8085$
* **$w_1$ 업데이트**:
    * $w_1^{new} = w_1 - \eta \cdot \frac{\partial L}{\partial w_1} = 0.5 - (0.5 \times -0.0068) = 0.5 + 0.0034 = 0.5034$

**결과**: 오차에 대한 책임이 음수(-)였던 가중치들은 아주 약간씩 **증가**했습니다. 이 작은 업데이트가 수만, 수백만 번 반복되면서 모델은 점차 정답에 가까운 예측을 하게 됩니다. 이것이 바로 딥러닝의 '학습'입니다.

