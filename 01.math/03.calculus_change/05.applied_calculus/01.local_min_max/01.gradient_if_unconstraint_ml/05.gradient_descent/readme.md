

### 1단계: 그래디언트의 진짜 출발점 - 미분 (1차원)

미분, 즉 $f'(x)$는 함수 $f(x)$ 위의 한 점 $x$에서 그은 **접선의 기울기**를 의미합니다.

* $f'(x) > 0$ 이면: 그 지점에서 함수는 **증가**하고 있습니다 (오르막길).
* $f'(x) < 0$ 이면: 그 지점에서 함수는 **감소**하고 있습니다 (내리막길).
* $|f'(x)|$ 값이 클수록: 그 지점의 경사가 **가파릅니다**.

이것이 핵심입니다. **미분은 '변화의 방향과 정도'를 알려줍니다.**


### 2단계: 다차원으로의 확장 - 편미분과 그래디언트

신경망의 손실(Loss) 함수 $L$은 수백만 개의 가중치($w_1, w_2, ..., w_n$)를 변수로 갖는 다변수 함수 $L(w_1, w_2, ...)$ 입니다. 여기서는 **편미분(Partial Derivative)** 이 필요합니다.

* **편미분 $\frac{\partial L}{\partial w_1}$ 의 의미**: 다른 모든 가중치($w_2, w_3, ...$)는 고정시킨 채, 오직 **$w_1$만 아주 살짝 움직였을 때** 손실 함수 $L$이 얼마나 변하는지를 측정한 값입니다. 즉, **$w_1$ 축 방향으로의 기울기** 입니다.

**그래디언트(Gradient)** 는 이 모든 편미분들을 하나의 벡터로 묶어놓은 것입니다.

$$
\nabla L = \begin{pmatrix}
\frac{\partial L}{\partial w_1} \\
\frac{\partial L}{\partial w_2} \\
\vdots \\
\frac{\partial L}{\partial w_n}
\end{pmatrix}
$$

그래디언트 $ \nabla L $은 이제 '하나의 벡터'이지만, 그 안에는 **모든 축 방향으로의 기울기 정보**가 전부 담겨 있습니다.


### 3단계: 수학적 핵심 - 왜 그래디언트가 '가장 가파른 오르막길'인가?

왜 저 벡터 $ \nabla L $가 가장 가파른 증가 방향을 가리킬까요? 이 질문은 **방향미분계수(Directional Derivative)**로 답할 수 있습니다.

* **질문**: 임의의 방향(크기가 1인 벡터 $\mathbf{u}$)으로 한 걸음 갔을 때의 기울기는 얼마일까요?
* **답**: 그 기울기(방향미분계수 $D_{\mathbf{u}}L$)는 그래디언트 벡터와 방향 벡터의 **내적(dot product)**으로 계산됩니다.

> $D_{\mathbf{u}}L = \nabla L \cdot \mathbf{u}$

내적 공식에 따라, $D_{\mathbf{u}}L = |\nabla L| |\mathbf{u}| \cos(\theta)$ 입니다. (여기서 $\theta$는 두 벡터 사이의 각도)

우리는 '가장 가파른' 기울기, 즉 $D_{\mathbf{u}}L$의 최댓값을 찾고 싶습니다. $|\nabla L|$은 현재 위치에서 정해진 상수이고, $|\mathbf{u}|=1$ 이므로, 이 값은 오직 $\cos(\theta)$에 의해 결정됩니다.

$\cos(\theta)$는 **$\theta=0$일 때 최댓값 1**을 갖습니다.

$\theta=0$이라는 것은 방향 벡터 $\mathbf{u}$와 그래디언트 벡터 $\nabla L$이 **완전히 같은 방향**을 가리킨다는 의미입니다.

**결론**:
임의의 방향 중 기울기가 최대가 되는 방향은 그래디언트 벡터($\nabla L$)와 같은 방향일 때입니다. 따라서, **그래디언트 벡터는 그 정의 자체로 '가장 가파른 오르막길의 방향과 크기'를 담고 있습니다.**


### 최종 정리: 모든 조각 맞추기

1.  **출발점(미분)**: 1차원에서 기울기는 '변화의 방향과 정도'를 의미합니다.
2.  **다차원 확장(그래디언트)**: 각 변수 방향의 기울기(편미분)를 모두 모아 하나의 벡터, **그래디언트($\nabla L$)** 를 만듭니다.
3.  **수학적 증명**: **방향미분계수** 를 통해, 그래디언트 벡터($\nabla L$)가 수학적으로 **가장 가파른 오르막길(Loss가 가장 크게 증가하는 방향)** 임을 증명했습니다.
4.  **경사 하강법**: 우리는 Loss를 줄이고 싶으므로, 가장 가파른 오르막길($\nabla L$)의 **정반대 방향**($-\nabla L$)으로 아주 조금씩($\text{학습률} \times$) 나아가며 Loss를 최소화합니다.

이것이 바로 `새로운 가중치 = 기존 가중치 - 학습률 * ∇L` 이라는 공식이 작동하는 수학적 원리입니다.

