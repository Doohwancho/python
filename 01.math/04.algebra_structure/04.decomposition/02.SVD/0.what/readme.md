# SVD와 고유값 분해(Eigendecomposition)의 관계

**SVD(특이값 분해)** 는 **고유값(Eigenvalue) 분해**와 매우 깊고 직접적인 관련이 있습니다.

- 고유값 분해 (Eigendecomposition): 정사각형 행렬 A에 대해서만 가능하며, A=PDP−1 형태로 분해합니다. P는 고유벡터(eigenvectors)로 이루어진 행렬, D는 고유값(eigenvalues)으로 이루어진 대각행렬입니다.
- SVD (Singular Value Decomposition): 모든 행렬 A (예: 직사각형 이미지)에 대해 A=UΣVT 형태로 분해합니다.

사실 SVD는 고유값 분해를 모든 행렬(정사각형이 아니어도 되는)에 적용할 수 있도록 일반화한 **'끝판왕' 버전**이라고 생각할 수 있습니다.


## a. SVD와 고유값 분해의 연결고리

* **고유값 분해**: **정사각형 행렬** $A$에 대해서만 $A = PDP^{-1}$ 형태로 분해합니다.
* **SVD**: **모든 $m \times n$ 행렬** $A$에 대해 $A = U\Sigma V^T$ 형태로 분해합니다.

둘의 연결고리는 $A$와 관련된 특별한 정사각형 대칭행렬인 **$A^T A$** 와 **$AA^T$** 를 통해 만들어집니다. 이 행렬들을 고유값 분해하면 SVD의 구성요소들이 나타납니다.

* **$V$의 열벡터들**: 행렬 **$A^T A$의 고유벡터(eigenvectors)** 입니다.
* **$U$의 열벡터들**: 행렬 **$AA^T$의 고유벡터(eigenvectors)** 입니다.
* **특이값($\sigma_i$)**: $A^T A$ (또는 $AA^T$)의 **고유값($\lambda_i$)들의 양의 제곱근**입니다. 즉, $\sigma_i = \sqrt{\lambda_i}$ 입니다.

결론적으로, SVD는 행렬 $A$와 직접 관련된 $A^T A$와 $AA^T$의 고유값과 고유벡터를 찾아내는 과정입니다.


## b. 코드와 "Eigenface" 개념 해석

이 관계를 바탕으로 제공해주신 Python 코드를 해석할 수 있습니다.

### 1. `U` 행렬: 이미지의 '세로 성분' (Eigenvectors of $AA^T$)

* 코드의 `U[:, i]`는 이미지의 **세로 구조**를 나타내는 기본적인 패턴(얼굴의 윤곽, 눈/코/입의 수직적 배치 등)을 담고 있습니다.
* 각 `U`의 열벡터 $\vec{u}_i$는 서로 직교하며, 이미지의 **열공간(Column Space)** 을 형성하는 **기저(basis)** 역할을 합니다.

### 2. `V` 행렬: 이미지의 '가로 성분' (Eigenvectors of $A^T A$)

* 코드의 `Vt[i, :]` (즉, $V$의 열벡터 $\vec{v}_i$)는 이미지의 **가로 구조**를 나타내는 기본적인 패턴(헤어 라인, 눈썹, 입술의 가로선 등)을 담고 있습니다.
* **바로 이 $V$의 열벡터들이 PCA에서 말하는 "고유얼굴(Eigenface)"의 본질**입니다. 이들은 `(이미지 데이터)^T (이미지 데이터)` 형태인 공분산 행렬의 고유벡터에 해당하기 때문입니다.
* 이 벡터들은 이미지의 **행공간(Row Space)** 에 대한 **기저** 역할을 합니다.

### 3. `s` 벡터 ($\Sigma$ 행렬): 각 성분의 '중요도'

* `s[j]`로 표현되는 특이값(singular value) $\sigma_j$는 **j번째 성분(U와 V의 j번째 벡터 쌍)이 원본 이미지를 구성하는 데 얼마나 중요한지**를 나타내는 '가중치' 또는 '에너지'입니다.
* 특이값이 큰 순서대로 정렬되므로, 앞의 몇 개의 성분만으로도 원본 이미지의 특징 대부분을 표현할 수 있습니다.

### 4. 재구성 (Reconstruction)

`reconstruction += s[j] * np.outer(U[:, j], Vt[j, :])`

이 코드는 SVD의 가장 중요한 의미인 "원본 이미지는 각 성분들의 합으로 표현된다"를 보여줍니다.
$$
\text{Image} (A) = \sum_{j=1}^{r} \sigma_j (\vec{u}_j \otimes \vec{v}_j) = \sigma_1 (\vec{u}_1 \otimes \vec{v}_1) + \sigma_2 (\vec{u}_2 \otimes \vec{v}_2) + \dots
$$
*($\otimes$는 외적(outer product)을 의미)*

즉, 복잡한 이미지($A$)를 서로 직교하는 **핵심적인 패턴들($U, V$)** 과 각 패턴의 **중요도($\Sigma$)** 로 분해하는 것이 SVD의 본질입니다. 이 때문에 데이터 압축, 노이즈 제거, 그리고 "Eigenface"와 같은 머신러닝 특징 추출 기법의 근간이 됩니다.

