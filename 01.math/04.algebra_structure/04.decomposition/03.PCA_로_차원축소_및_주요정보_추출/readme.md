PCA(Principal Component Analysis, 주성분 분석)는 **고차원 데이터의 본질적인 구조(뼈대)를 파악하여 저차원으로 축소시키는 대표적인 차원 축소(Dimensionality Reduction) 기법**입니다.

핵심 아이디어는 데이터가 가장 넓게 퍼져있는(분산이 가장 큰) 방향을 새로운 '주축'으로 삼아, 이 축들로 데이터를 다시 표현하는 것입니다.


## 1. PCA의 핵심 목표: 정보 손실 최소화

여러 개의 특성(feature)을 가진 고차원 데이터(예: 수십 개의 항목으로 평가한 고객 만족도)를 2차원이나 3차원으로 축소하여 시각화하거나 분석하고 싶을 때, 어떻게 해야 정보 손실을 최소화할 수 있을까요?

PCA는 **데이터의 분산(Variance)을 정보의 양**이라고 가정합니다. 따라서 분산이 가장 큰 방향을 찾아 그 축에 데이터를 투영(projection)하면 원본 데이터의 정보를 가장 많이 보존할 수 있습니다.



## 2. PCA의 단계별 과정

PCA는 다음 5단계로 진행됩니다.

#### **1단계: 데이터의 중심 맞추기 (Mean-Centering)**

-   각 데이터의 특성(feature, 각 열)별로 평균을 구한 뒤, 모든 데이터에서 해당 평균값을 빼줍니다.
-   이 과정을 통해 데이터 전체의 무게 중심이 원점(0,0,...)에 오게 됩니다. 이는 분산을 올바르게 계산하기 위한 전처리 과정입니다.

#### **2단계: 공분산 행렬 계산 (Calculate Covariance Matrix)**

-   중심이 맞춰진 데이터가 각 축(특성)에 대해 어떻게 함께 변하는지를 나타내는 **공분산 행렬**을 계산합니다. 데이터 행렬을 $X$라고 할 때, 공분산 행렬은 $X^T X$와 비례합니다.
-   이 행렬의 각 요소는 특성 간의 상관관계를 담고 있습니다.

#### **3. 공분산 행렬의 고유값 분해 (Eigendecomposition)**

-   **PCA의 가장 핵심적인 단계입니다.** 2단계에서 구한 공분산 행렬을 고유값 분해합니다.
-   이때 얻어지는 결과물들은 다음과 같은 의미를 가집니다.
    -   **고유벡터 (Eigenvectors)**: 데이터의 분산이 가장 큰 방향을 나타내는 **새로운 축**, 즉 **주성분(Principal Components, PC)**이 됩니다.
    -   **고유값 (Eigenvalues)**: 각 고유벡터(주성분) 방향으로 데이터가 **얼마나 큰 분산(중요도)을 가지는지**를 나타내는 값입니다.

#### **4. 주성분 선택 (Select Principal Components)**

-   고유값의 크기 순서대로 고유벡터(주성분)를 정렬합니다. 고유값이 가장 큰 고유벡터가 제1 주성분(PC1), 두 번째가 제2 주성분(PC2)이 됩니다.
-   차원을 축소하기 위해, 이 중에서 상위 $k$개의 주성분만을 선택합니다. (예: 10차원 -> 2차원으로 축소하고 싶다면 PC1, PC2만 선택)

#### **5. 데이터 변환 (Project Data)**

-   1단계의 중심화된 데이터를 4단계에서 선택한 $k$개의 주성분(고유벡터)으로 이루어진 새로운 공간에 투영(dot product)합니다.
-   결과적으로, 원래의 고차원 데이터는 선택된 주성분 축을 기준으로 하는 새로운 저차원 좌표값으로 변환됩니다.


## 3. SVD와 PCA의 관계

실제로는 공분산 행렬을 계산하는 과정에서 발생할 수 있는 수치적 불안정성 때문에, **SVD를 이용해 PCA를 수행하는 경우가 더 많습니다.**

-   1단계에서 중심을 맞춘 데이터 행렬 $X$를 바로 SVD($X = U\Sigma V^T$)하면,
-   여기서 얻어지는 **오른쪽 특이벡터 행렬 $V$의 열벡터들이 바로 주성분(Principal Components)과 동일합니다.**

즉, **데이터 행렬을 SVD하는 것이 공분산 행렬을 고유값 분해하는 것과 수학적으로 같은 결과를 낳습니다.**


## 4. 결론

PCA는 복잡한 데이터를 '가장 중요한 축' 순서대로 재정렬하여, 핵심적인 구조만 남기고 차원을 줄이는 강력한 도구입니다. 그 근본에는 데이터의 분산 방향을 찾기 위한 **고유값 분해** 또는 **SVD**라는 선형대수의 핵심 원리가 자리 잡고 있습니다.


# Q. 주성분인 PC1, PC2 어떻게 구함?

## 주성분(PC1, PC2)의 수학적 계산 과정

PCA의 목표는 **데이터의 분산을 최대로 보존하는 새로운 직교 기저(basis)를 찾는 것** 이며, 이는 **공분산 행렬의 고유값 분해**를 통해 달성됩니다.

### 1. 데이터 준비 및 중심화

$d$차원의 데이터 포인트가 $n$개 있다고 할 때, 데이터셋은 벡터의 집합 $D = \{\vec{x}_1, \vec{x}_2, \dots, \vec{x}_n\}$ 으로 표현할 수 있습니다.

가장 먼저 데이터의 평균 벡터 $\vec{\mu}$를 계산합니다.
$$\vec{\mu} = \frac{1}{n} \sum_{i=1}^{n} \vec{x}_i$$
그 다음, 모든 데이터 포인트에서 이 평균 벡터를 빼서 데이터를 원점 중심화합니다. 중심화된 데이터 벡터를 $\vec{z}_i$라고 하겠습니다.
$$\vec{z}_i = \vec{x}_i - \vec{\mu}$$

---

### 2. 공분산 행렬 계산

다음으로 중심화된 데이터를 사용하여 공분산 행렬(Covariance Matrix) $C$를 계산합니다. 공분산 행렬은 데이터의 **각 차원들이 서로 어떻게 연관되어 흩어져 있는지**를 나타냅니다.

$$C = \frac{1}{n-1} \sum_{i=1}^{n} \vec{z}_i \vec{z}_i^T$$
여기서 $\vec{z}_i \vec{z}_i^T$는 외적(outer product)이며, 결과적으로 $C$는 $d \times d$ 크기의 대칭행렬이 됩니다.

---

### 3. 고유값 분해 (Eigendecomposition)

**이 단계가 PCA의 핵심입니다.** 공분산 행렬 $C$에 대한 고유값 문제를 풉니다.
$$C\vec{v} = \lambda\vec{v}$$
이 방정식을 만족하는 해를 찾으면 다음과 같은 의미를 갖습니다.

-   $\lambda$ (**고유값, Eigenvalue**): 스칼라 값으로, 대응하는 고유벡터 방향으로 데이터가 퍼진 정도, 즉 **분산의 크기**를 나타냅니다. 이 값이 클수록 해당 방향이 더 중요합니다.
-   $\vec{v}$ (**고유벡터, Eigenvector**): 크기가 1인 단위벡터로, 공분산 행렬 $C$를 곱해도 방향이 변하지 않는 **특별한 방향**을 나타냅니다. 이 방향이 바로 **주성분(Principal Component)** 이 됩니다.

$d$차원 데이터에 대해 우리는 $d$개의 고유값-고유벡터 쌍 $(\lambda_1, \vec{v}_1), (\lambda_2, \vec{v}_2), \dots, (\lambda_d, \vec{v}_d)$를 얻게 됩니다.


### 4. 주성분 선택

마지막으로, 계산된 고유값들을 크기 순으로 정렬합니다.
$$\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_d$$
이 순서에 맞춰 해당 고유벡터들도 정렬합니다.

-   **제1 주성분 (PC1)**: 가장 큰 고유값 $\lambda_1$에 대응하는 고유벡터 $\vec{v}_1$입니다. 이 방향이 데이터의 분산을 가장 많이 설명하는 축입니다.
    $$
    \text{PC1} = \vec{v}_1
    $$
-   **제2 주성분 (PC2)**: 두 번째로 큰 고유값 $\lambda_2$에 대응하는 고유벡터 $\vec{v}_2$입니다. 이 방향은 PC1에 직교하면서, 남은 분산을 가장 많이 설명하는 축입니다.
    $$
    \text{PC2} = \vec{v}_2
    $$
결론적으로, **PC1과 PC2는 데이터 공분산 행렬의 고유값 분해를 통해 얻어지는, 가장 큰 두 개의 고유값에 해당하는 고유벡터들**입니다.