# Q. what is decomposition?
`Ax=b`에서 행렬 A를 분해하는 것.

행렬 $A$를 분해하면 그 행렬이 나타내는 **선형 변환(linear transformation)의 본질적인 기하학적 의미**를 파악하거나, 복잡한 행렬 계산(예: 역행렬, 행렬식)을 더 쉽게 할 수 있습니다.

분해하는 방법으로는 크게
1. eigen decomposition
2. SVD
...가 있다.

# **Q. 고유값 분해 (Eigen Decomposition) vs. 특이값 분해 (SVD)**

### **고유값 분해 (Eigen Decomposition)**

-   **수식**: $A = PDP^{-1}$
-   **조건**: **정방행렬(square matrix)** $A$에 대해서만 적용 가능하며, 행렬 $A$가 선형 독립인 고유벡터(eigenvector)들을 충분히 가지고 있을 때 가능합니다.
-   **구성 요소**:
    -   $P$: 행렬 $A$의 **고유벡터(eigenvector)** 들을 열벡터로 갖는 행렬. 이 벡터들은 $A$에 의해 변환될 때 **방향이 변하지 않고 크기만 변하는** 벡터들입니다.
    -   $D$: 대응되는 **고유값(eigenvalue)** $\lambda$를 대각 원소로 갖는 **대각행렬(diagonal matrix)**. 고유값은 고유벡터가 늘어나거나 줄어드는 **배율(scaling factor)** 을 의미합니다.
-   **기하학적 의미**: 어떤 벡터에 행렬 $A$를 곱하는 변환은, 좌표계를 $P^{-1}$로 바꾸고($P$의 열벡터인 고유벡터들을 기저로 삼고), 바뀐 좌표계에서 각 축 방향으로 $D$만큼(고유값 배율만큼) 늘리거나 줄인 다음, 다시 원래 좌표계 $P$로 돌아오는 것과 같습니다. 즉, **변환의 '축'과 '축 방향의 신축률'** 을 찾아내는 과정입니다.

$$Av = \lambda v$$

### **특이값 분해 (Singular Value Decomposition, SVD)**

-   **수식**: $A = U\Sigma V^T$
-   **조건**: **모든 행렬(m x n 크기의 직사각 행렬 포함)** 에 적용 가능합니다. 훨씬 일반적인 분해 방법입니다.
-   **구성 요소**:
    -   $U$: **왼쪽 특이벡터(left singular vectors)** 를 열로 갖는 **직교행렬(orthogonal matrix)**. 변환 후 결과 공간(output space)의 직교 기저(orthonormal basis)를 이룹니다.
    -   $\Sigma$: **특이값(singular value)** $\sigma$를 대각 원소로 갖는 **대각행렬**. 특이값은 변환 과정에서의 **순수한 확대/축소율**을 나타내며, 항상 0 또는 양수입니다.
    -   $V^T$: **오른쪽 특이벡터(right singular vectors)** 를 행으로 갖는 **직교행렬**($V$의 전치). 변환 전 원래 공간(input space)의 직교 기저를 이룹니다.
-   **기하학적 의미**: 모든 선형 변환 $A$는 **회전($V^T$) → 확대/축소($\Sigma$) → 다시 회전($U$)** 이라는 세 가지 기본 동작의 조합으로 설명할 수 있다는 의미입니다. 차원이 다른 공간으로의 변환도 설명할 수 있습니다.

| 구분 | 고유값 분해 (Eigen Decomposition) | 특이값 분해 (SVD) |
| :--- | :--- | :--- |
| **대상 행렬** | 정방행렬 ($n \times n$) | 모든 행렬 ($m \times n$) |
| **분해 형태** | $A = PDP^{-1}$ | $A = U\Sigma V^T$ |
| **핵심 요소** | 고유값($\lambda$), 고유벡터($v$) | 특이값($\sigma$), 특이벡터($u, v$) |
| **기하학적 의미** | 변환의 **'신축 축'** 과 **'신축률'** | **회전 → 신축 → 회전** |
| **구성 행렬** | $P$는 일반적으로 직교행렬이 아님 | $U, V$는 항상 직교행렬 |


# **Q. 주성분 분석 (PCA)과 행렬 분해의 관계**

PCA는 행렬 분해를 이용해 데이터 차원을 축소하는 기법.\
PCA는 데이터의 분산이 가장 큰 방향(주성분)을 찾는 것을 목표로 합니다.


데이터 행렬 $X$가 있다고 할 때, PCA를 수행하는 것은 다음 두 가지와 같습니다.

1.  **데이터의 공분산 행렬(Covariance Matrix)을 고유값 분해**하는 것.
    -   공분산 행렬 $X^T X$는 정방행렬이므로 고유값 분해가 가능합니다.
    -   이때 얻어지는 **고유벡터(eigenvector)가 바로 주성분(Principal Components)**, 즉 데이터 분산이 가장 큰 방향 벡터가 됩니다.
    -   **고유값(eigenvalue)은 각 주성분 방향으로 데이터가 가진 분산의 크기**를 나타냅니다. 고유값이 클수록 더 중요한 주성분입니다.

2.  **데이터 행렬 $X$ 자체를 특이값 분해(SVD)** 하는 것.
    -   데이터 행렬 $X$에 SVD를 적용하면($X = U\Sigma V^T$), 공분산 행렬의 고유값 분해와 매우 밀접한 결과를 얻습니다.
    -   $X$를 SVD해서 얻은 **오른쪽 특이벡터 행렬 $V$의 열벡터들이 정확히 주성분**이 됩니다.
    -   실제로는 수치적 안정성 때문에 공분산 행렬을 직접 계산하기보다 SVD를 이용하는 경우가 많습니다.

**결론적으로, PCA는 데이터의 핵심 구조(분산이 큰 방향)를 찾기 위해 고유값 분해 또는 SVD라는 강력한 행렬 분해 도구를 사용하는 응용 기술**이라고 할 수 있습니다.


