# A. 개념 
## a-1. 최소제곱법(Least Squares)에 숨겨진 선형대수

단순히 "점들을 대충 잇는 것"처럼 보이는 최소제곱법은 사실 **4대 기본 부분공간**과 **직교성**이라는 선형대수의 핵심 개념을 바탕으로 한 매우 정교한 해법입니다.


최소제곱법: 모든 회귀(Regression) 모델의 근간

### a-2. 회귀(Regression)란?

**회귀**는 하나 이상의 **독립 변수(원인, $X$)** 를 사용하여 **연속적인 종속 변수(결과, $y$)** 의 값을 예측하는 모델을 만드는 기법입니다. 가장 단순한 모델은 다음과 같은 **선형 회귀** 모델입니다.

$$ \text{예측값} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots $$

머신러닝 모델을 '학습'시키는 것은, 수많은 실제 데이터를 보고 예측이 실제 값과 가장 비슷해지도록 하는 **최적의 계수($\beta_0, \beta_1, \dots$)** 를 찾아내는 과정입니다.

### a-3. '최적'이란 무엇인가? -> 최소제곱법의 등장

'최적'이란 **실제값과 예측값의 차이(오차, Error)를 가장 작게** 만드는 것을 의미합니다. 오차는 양수일 수도, 음수일 수도 있으므로, 수학자들은 오차를 **제곱**하여 모두 양수로 만든 뒤, 그 합을 최소화하는 방법을 고안했습니다.

$$ \text{총 오차} = \sum (\text{실제값}_i - \text{예측값}_i)^2 $$

**"제곱의 합(Sum of Squares)을 최소화(Least)한다"** — 이것이 바로 **최소제곱법(Least Squares)** 의 핵심 원리입니다.

우리가 선형대수에서 배운 $A^T A \hat{x} = A^T \vec{b}$ 라는 정규방정식(Normal Equation)이 바로 이 **'총 오차 제곱의 합'을 최소화하는 계수 $\hat{x}$를 찾는 수학적 해법**입니다. 따라서 최소제곱법은 모든 회귀 모델의 철학적, 수학적 근간이 됩니다.



# B. manim 해석 
## 1. 문제의 재구성: "해가 없는" 선형 시스템 $A\vec{x} = \vec{b}$

주어진 데이터 점 $(1, 15), (2, 22), \dots, (5, 43)$을 모두 통과하는 단 하나의 직선 $y = c + mx$는 존재하지 않습니다. 이를 행렬로 표현하면 다음과 같은 선형 시스템이 되며, 이 시스템의 **정확한 해 $\vec{x}$는 존재하지 않습니다.**

$$
\underbrace{
\begin{bmatrix}
1 & 1 \\
1 & 2 \\
1 & 3 \\
1 & 4 \\
1 & 5
\end{bmatrix}
}_{A}
\underbrace{
\begin{bmatrix}
c \\
m
\end{bmatrix}
}_{\vec{x}}
=
\underbrace{
\begin{bmatrix}
15 \\
22 \\
28 \\
35 \\
43
\end{bmatrix}
}_{\vec{b}}
$$

기하학적으로 이는 "결과 벡터 $\vec{b}$가 행렬 $A$의 **열공간(Column Space)** 안에 존재하지 않는다"는 의미입니다.

## 2. 해결의 열쇠: 직교성을 이용한 정사영(Orthogonal Projection)

정확한 해를 찾을 수 없다면, 우리는 **'가장 비슷한' 해**를 찾아야 합니다.

* **아이디어**: 우리가 도달할 수 없는 목표 지점 $\vec{b}$ 대신, 도달 가능한 영역(**Column Space**) 안에서 $\vec{b}$와 **가장 가까운 지점 $\vec{p}$** 를 찾습니다.

* **핵심 원리**: 가장 가까운 점 $\vec{p}$는 $\vec{b}$를 $A$의 Column Space에 **정사영(Orthogonal Projection)**시킨 벡터입니다. 이때, 오차 벡터($\vec{e} = \vec{b} - \vec{p}$)는 Column Space 전체와 **직교(perpendicular)** 해야 합니다.


## 3. 정규방정식(Normal Equation)의 유도

오차 벡터 $\vec{e}$가 $A$의 Column Space와 직교한다는 것은, $\vec{e}$가 **Left Null Space** ($N(A^T)$)의 원소라는 의미입니다.

Left Null Space의 정의에 따라 다음이 성립합니다.
$$ A^T \vec{e} = \vec{0} $$

$\vec{e} = \vec{b} - A\hat{x}$ 를 대입하면, ($\hat{x}$는 우리가 찾으려는 근사해)
$$ A^T (\vec{b} - A\hat{x}) = \vec{0} $$

이 식을 정리하면 최소제곱법의 핵심 공식인 **정규방정식**이 탄생합니다.
$$ A^T A \hat{x} = A^T \vec{b} $$

## 4. 코드의 역할

Python 코드는 정확히 위의 과정을 수행합니다.

1.  `A = np.vstack(...)` : 데이터로부터 행렬 $A$를 만듭니다.
2.  `x_hat = np.linalg.inv(A.T @ A) @ A.T @ b` : 정규방정식을 직접 풀어서 최적의 근사해 $\hat{x} = \begin{bmatrix} c \\ m \end{bmatrix}$ 를 계산합니다.
3.  `plt.plot(...)` : 계산된 $c$와 $m$을 이용해 최적의 추세선을 그립니다.

결론적으로, 최소제곱법은 단순히 점들을 잇는 것이 아니라, **해가 없는 선형 시스템을 풀기 위해 목표 벡터를 열공간에 정사영하여 가장 오차가 적은 근사해를 찾는** 선형대수의 정교한 응용입니다.