# step13. feature extractor 

약간 CNN 앞단에 feature extract에서 conv1,2 layer만 똑 떼서 visualize한 것 같음 

창 1: Layer 1: Feature Maps (첫 번째 컨볼루션 레이어)
- 무엇을 보여주는가?: **저수준 특징(Low-level features)** 을 보여줍니다. 8개의 필터가 각각 원본 이미지에서 무엇을 '감지'했는지를 나타냅니다.
- 관찰 포인트:
    - 특정 방향의 선(Edge): 어떤 필터는 가로선에만 반응해서 고양이의 눈, 코, 수염의 가로 부분을 하얗게 표시할 것입니다. 또 다른 필터는 세로선(얼굴 윤곽선 등)에만 반응할 수 있습니다.
    - 밝기/어두움: 어떤 필터는 밝은 영역(예: 하얀 털)에 반응하고, 어떤 필터는 어두운 영역(예: 눈동자, 검은 무늬)에 반응합니다.
    - 단순한 질감: 특정 필터는 고양이 털의 오돌토돌한 질감 같은 단순한 패턴을 감지할 수 있습니다.
- 핵심: 각 필터는 자신만의 '전문 분야'를 가지고 원본 이미지를 스캔한 결과물입니다. 아직은 고양이의 '형태'라기보다는 '구성 요소'에 가깝습니다.


창 2: Layer 2: Feature Maps (두 번째 컨볼루션 레이어)
- 무엇을 보여주는가?: **중간 수준 특징(Mid-level features)** 을 보여줍니다. 16개의 필터가 각각 첫 번째 레이어의 특징 맵들을 조합해서 더 복잡한 것을 감지합니다.
- 관찰 포인트:
    - 단순 특징의 조합: 예를 들어, '가로선에 반응하는 필터'의 결과와 '세로선에 반응하는 필터'의 결과를 조합하여 '코너'나 '모서리'를 감지하는 필터가 있을 수 있습니다.
    - 곡선 및 형태: '짧은 대각선' 특징들을 여러 개 조합하여 '귀의 둥근 곡선'이나 '눈의 아치 형태'에 반응하는 필터가 나타나기 시작합니다.
    - 복잡한 텍스처: 단순한 질감을 조합하여 '털이 뭉친 패턴' 같은 더 복잡한 질감을 감지합니다.
- 핵심: 이제부터 이미지는 점점 추상화됩니다. 우리 눈에는 원본 고양이와는 전혀 다르게 보이지만, 모델에게는 '고양이의 귀'나 '고양이의 눈' 같은 의미 있는 정보가 담긴 중요한 지도입니다.

이 과정을 여러 번 반복하며 깊은 레이어로 갈수록, 모델은 최종적으로 '고양이'라는 매우 추상적이고 고수준의 개념을 학습하게 되는 것입니다.


# step14. from scratch 

## a. what 
step13. feature extractor에 `nn.Conv2d()` 함수를 from scratch 로 구현한 것.

## b. 원리 
이것도 예전에 10x10 이미지에서 edge detection할 때 썼던 3x3 kernel을 이미지 픽셀별로 for문 돌면서 matrix multiplication해서 특징 추출하는거랑 같은 원리\
...이나, 거기에 
1. 차원 확장(흑백 2d img -> 색깔 2d img(multiple channel))
2. 커널의 갯수 증가(한 종류 필터에서 여러 종류 필터로)

만 바뀐 것.

### b-1. 기존 edge 특징 검출을 convolution 연산으로 한 원리
1. 특수 안경(커널)을 쓰고 모니터(이미지)의 왼쪽 위부터 훑어보기 시작합니다. (슬라이딩)
2. 안경에 보이는 부분과 실제 화면 부분을 비교(원소별 곱셈)해서 그 결과를 종합(합산)합니다.
3. 이 과정을 모니터 전체에 대해 반복하면, 윤곽선만 하얗게 보이는 새로운 화면(Feature Map)이 만들어집니다.

### b-2. 현재 feature extractor 
색깔에 64x64 고양이 이미지 

1. 일단 Conv1을 저 특징 추출 돌리면 8개의 feature map이 나온다.
    - 기존에 에지 디텍션은 edge detect하는 3x3 kernel(filter라고 부름)을 1개써서, 각 픽셀에 곱한 결과 이미지 행렬이 1개 나오는거고,
    - 지금 색상 고양이 예제는, 3x3 kernel 8개(필터 8개) 써서 특징 추출한다. 그래서 행렬곱하면 feature map이 8개 생성된다.
    - 물론 이 3x3 kernel(필터) 8개는 일단 이미지에서 뭐가 중요한 특징인지 모르기 때문에, 다양한 종류의 필터를 먹인다.
        - 가로선이 중요할까? 세로선이 중요할까? 녹색 부분이 중요할까? 둥근 모양이 중요할까?
        - 모르니까 다양한 종류의 필터를 먹여보자.

```
1번 필터(안경): "나는 이미지에서 가로선만 찾겠다!"

2번 필터(안경): "나는 세로선만 찾겠다!"

3번 필터(안경): "나는 45도 대각선만 찾겠다!"

4.번 필터(안경): "나는 녹색에서 흰색으로 변하는 부분만 찾겠다!"

... (이하 생략) ...

8번 필터(안경): "나는 뭉툭한 점 형태만 찾겠다!"

작업 과정:

1번 전문가가 원본 이미지를 훑어서 '가로선 지도'를 보고서로 만듭니다. -> 특징 맵 1
2번 전문가가 똑같은 원본 이미지를 훑어서 '세로선 지도'를 보고서로 만듭니다. -> 특징 맵 2
...
8번 전문가가 똑같은 원본 이미지를 훑어서 '뭉툭한 점 지도'를 보고서로 만듭니다. -> 특징 맵 8

최종 결과물: 각각 다른 종류의 특징 정보를 담은 보고서 8장 = 특징 맵 8개
```

코드에서는 이렇게 구현되어있다.
```python
# out_channels=8 이 바로 "8명의 전문가(필터)를 사용하겠다"는 의미입니다.
self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)
```

from scratch 코드에서는 이렇게 구현되어있다.
```python
# (8, 1, 3, 3)의 첫 번째 숫자 8이 "8개의 필터를 만들겠다"는 의미입니다.
conv1_kernels = np.random.randn(8, 1, 3, 3) # (필터 개수, 입력 채널 수, 높이, 너비)
```



## c. 코드 설명 
1. convolve_2d 함수: 이 함수가 for 루프를 사용해 슬라이딩 윈도우를 직접 구현한, 컨볼루션의 가장 핵심적인 부분입니다. PyTorch에서는 이 과정이 고도로 최적화된 C++나 CUDA 코드로 내부에서 실행됩니다.
2. apply_conv_layer 함수: PyTorch의 nn.Conv2d 레이어 하나가 하는 일을 흉내 낸 함수입니다. 입력 채널이 여러 개일 때(예: conv2는 8개의 채널을 입력받음), 각 커널 역시 똑같은 수의 채널(깊이)을 가져야 합니다. 이 함수는 각 채널별로 컨볼루션을 수행하고 그 결과를 모두 합산하여 최종 출력 맵 하나를 만들어냅니다.
3. np.random.randn(...): model = SimpleCNN()을 대신하는 부분입니다. 실제 모델은 학습 가능한 가중치(커널)를 가지지만, 우리는 학습 과정은 구현하지 않으므로, 마치 학습 시작 직전의 모델처럼 랜덤한 값으로 커널을 초기화합니다.
4. 결과 이미지 해석: PyTorch 버전과 마찬가지로, 각 필터가 이미지의 특정 패턴에 반응한 결과를 보여줍니다. 하지만 한 가지 큰 차이점은, 이 커널들은 랜덤하게 생성되었기 때문에 학습된 모델처럼 '고양이의 눈'이나 '귀' 같은 의미 있는 특징을 포착하지는 못합니다. 대신, 각 랜덤 필터가 우연히 가지고 있는 패턴(예: 특정 방향의 선, 특정 색 조합)에 반응하는 모습을 볼 수 있습니다. 이를 통해 **"커널의 패턴과 유사한 이미지 영역이 활성화된다"** 는 컨볼루션의 근본 원리를 명확하게 확인할 수 있습니다.


